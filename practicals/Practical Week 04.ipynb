{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for Name Gender Classification\n",
    "\n",
    "We have already seen the following code for partitioning the data of name gender classification and feature extraction. The code is changed slightly so that the labels are numerical (0 for male, 1 for female). This is the format required for Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\Madhur\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\Madhur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "m = names.words('male.txt')\n",
    "f = names.words('female.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234) # Set the random seed to allow replicability\n",
    "names = ([(name,0) for name in m] +\n",
    "         [(name,1) for name in f])\n",
    "random.shuffle(names)\n",
    "train_names = names[1000:]\n",
    "devtest_names = names[500:1000]\n",
    "test_names = names[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_character(c):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    result = [0]*(len(alphabet)+1)\n",
    "    i = alphabet.find(c.lower())\n",
    "    if i >= 0:\n",
    "        result[i] = 1\n",
    "    else:\n",
    "        result[len(alphabet)] = 1 # character is out of the alphabet\n",
    "    return result\n",
    "\n",
    "def gender_features_n(word, n=2):\n",
    "    \"Return the one-hot encodings of the last n characters\"\n",
    "    features = []\n",
    "    for i in range(n):\n",
    "        if i < len(word):\n",
    "            features = one_hot_character(word[-i-1]) + features\n",
    "        else:\n",
    "            features = one_hot_character(' ') + features\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_features_n(\"Mary\", n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's determine the number of features so that we can use this information when we design the neural network\n",
    "len(gender_features_n(\"Mary\", n=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Simple Neural Network\n",
    "Design a simple neural network that has 54 input cells (that's the number of gender features for $n=2$, as we have seen above), and one output cell (without a hidden layer). The output cell will be used to classify the name between male (output=0) and female (output=1). This is therefore an instance of **binary classification**. Pay attention to the right activation function! This simple model, without hidden layers, is equivalent to a **logistic regression** classifier. The model summary should look like this:\n",
    "\n",
    "```\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "dense_2 (Dense)              (None, 1)                 55        \n",
    "=================================================================\n",
    "Total params: 55\n",
    "Trainable params: 55\n",
    "Non-trainable params: 0\n",
    "```\n",
    "\n",
    "\n",
    "Compile the model and provide the right loss function. Use `'rmsprop'` as the optimiser, and include `'accuracy'` as an evaluation metric. \n",
    "Run the network **for 100 epochs** using batch size of 100, and observe the results. \n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. What is the best result on the validation set?\n",
    "2. At the epoch with best result on the validation set, what is the result on the training set?\n",
    "3. Is the system overfitting? Justify your answer.\n",
    "4. Do we really need 100 epochs? Do we need more than 100 epochs? would the system run better with less epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached https://files.pythonhosted.org/packages/fe/36/7c7c9f106e3026646aa17d599b817525b139e2870f75b532318573cbabd4/tensorflow-2.8.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from tensorflow) (41.0.1)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/6b/44f7f8f1e110027cf88956b59f2fad776cca7e1704396d043f89effd3a0e/typing_extensions-4.1.1-py3-none-any.whl\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from tensorflow) (2.9.0)\n",
      "Collecting gast>=0.2.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/5f/1c/b59500a88c5c3d9d601c5ca62b9df5e0964764472faed82a182958a922c5/gast-0.5.3-py3-none-any.whl\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.4.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/2c/03/e3e19d3faf430ede32e41221b294e37952e06acc96781c417ac25d4a0324/absl_py-1.0.0-py3-none-any.whl\n",
      "Collecting tensorboard<2.9,>=2.8 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/fd/67c61276de025801cfa8a1b9af2d7c577e7f27c17b6bff2baca20bf03543/tensorboard-2.8.0-py3-none-any.whl\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/da/4e/bf276e4dd76c6db2bd8d0e1e0e37f904042fb29269abe375726cf579d4c0/tensorflow_io_gcs_filesystem-0.24.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting numpy>=1.20 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/22/a3/3a5469ebaca59100e50b4300dd011eed943f2aad7c6a80a07966b985e2c6/numpy-1.21.5-cp37-cp37m-win_amd64.whl\n",
      "Collecting keras<2.9,>=2.8.0rc0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/2f/eb9391bdcba2693cc8396f244bd3b4512bcd1123c2ea06f4dfcf50dc5ce9/keras-2.8.0-py2.py3-none-any.whl\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3f/89/4920de0ec2ccbcb26e3f8941c20ba797807d49b11d4424d71cf001c646f7/grpcio-1.44.0-cp37-cp37m-win_amd64.whl\n",
      "Collecting keras-preprocessing>=1.1.1 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl\n",
      "Collecting flatbuffers>=1.12 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/3d/d0/26033c70d642fbc1e35d3619cf3210986fb953c173b1226709f75056c149/flatbuffers-2.0-py2.py3-none-any.whl\n",
      "Collecting tf-estimator-nightly==2.8.0.dev2021122109 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/a7/f1/f89e097f377b163856076f167baf149b010df3bbf425d2c06276048e2051/tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl\n",
      "Collecting libclang>=9.0.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/09/98786c8eaae694c63a9305667d4e60faec3d5a3b7c32fdd30ce02c470cab/libclang-13.0.0-py2.py3-none-win_amd64.whl (13.9MB)\n",
      "Collecting protobuf>=3.9.2 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/b3/41/5e2dfb15fdae38be5dd62be30f9fd40c57e50bd56ee2c276f6230c57a3ad/protobuf-3.19.4-cp37-cp37m-win_amd64.whl\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.33.4)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/66/12/eb8e0254e84f47deb4bd65858aef26f93fb4786091442d6bd2e86a5843d3/google_auth-2.6.0-py2.py3-none-any.whl (156kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.15.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.22.0)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/d4/2c7f83915d437736996b2674300c6c4b578a6f897f34e40f5c04db146719/Markdown-3.3.6-py3-none-any.whl (97kB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/68/e8ecfac5dd594b676c23a7f07ea34c197d7d69b3313afdf8ac1b0a9905a2/tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/74/69/5747a957f95e2e1d252ca41476ae40ce79d70d38151d2e494feb7722860c/tensorboard_data_server-0.6.1-py3-none-any.whl\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/b1/0e/0636cc1448a7abc444fb1b3a63655e294e0d2d49092dc3de05241be6d43c/google_auth_oauthlib-0.4.6-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3.6\" (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/30/ab/8fd9e88e6fa5ec41afca995938bbefb72195278e0cfc5bd76a4f29b23fb2/rsa-4.8-py3-none-any.whl\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/19/99/ace1769546388976b45e93445bb04c6df95e96363f03fbb56f916da5ebde/cachetools-5.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
      "Collecting importlib-metadata>=4.4; python_version < \"3.10\" (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/92/f2/c48787ca7d1e20daa185e1b6b2d4e16acd2fb5e0320bc50ffc89b91fa4d7/importlib_metadata-4.11.3-py3-none-any.whl\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/bb/5deac77a9af870143c684ab46a7934038a53eb4aa975bc0687ed6ca2c610/requests_oauthlib-1.3.1-py2.py3-none-any.whl\n",
      "Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\madhur\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (0.5.1)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/46/5ee2475e1b46a26ca0fa10d3c1d479577fde6ee289f8c6aa6d7ec33e31fd/oauthlib-3.2.0-py3-none-any.whl (151kB)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Madhur\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built termcolor\n",
      "Installing collected packages: termcolor, google-pasta, typing-extensions, gast, astunparse, absl-py, grpcio, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, numpy, protobuf, importlib-metadata, markdown, tensorboard-plugin-wit, tensorboard-data-server, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow-io-gcs-filesystem, keras, opt-einsum, keras-preprocessing, flatbuffers, tf-estimator-nightly, libclang, tensorflow\n",
      "  Found existing installation: numpy 1.16.4\n",
      "    Uninstalling numpy-1.16.4:\n",
      "      Successfully uninstalled numpy-1.16.4\n",
      "  Found existing installation: importlib-metadata 0.17\n",
      "    Uninstalling importlib-metadata-0.17:\n",
      "      Successfully uninstalled importlib-metadata-0.17\n",
      "Successfully installed absl-py-1.0.0 astunparse-1.6.3 cachetools-5.0.0 flatbuffers-2.0 gast-0.5.3 google-auth-2.6.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.44.0 importlib-metadata-4.11.3 keras-2.8.0 keras-preprocessing-1.1.2 libclang-13.0.0 markdown-3.3.6 numpy-1.21.5 oauthlib-3.2.0 opt-einsum-3.3.0 protobuf-3.19.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.8 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.8.0 tensorflow-io-gcs-filesystem-0.24.0 termcolor-1.1.0 tf-estimator-nightly-2.8.0.dev2021122109 typing-extensions-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Write your model here\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(1, activation='sigmoid', input_shape=(54,)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "loss='categorical_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "            metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_1 (Dense)             (None, 1)                 55        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(write additional code for the partition of the data, your experiments, and your analysis. Write the answers to the questions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = [gender_features_n(x[0]) for x in train_names]\n",
    "partial_x_train = [gender_features_n(x[0]) for x in devtest_names]\n",
    "\n",
    "y_val = [x[1] for x in train_names]\n",
    "partial_y_train = [x[1] for x in devtest_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5/5 [==============================] - 28s 6s/step - loss: 0.0000e+00 - binary_accuracy: 0.4080 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4045\n",
      "Epoch 2/100\n",
      "5/5 [==============================] - 1s 314ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3960 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4003\n",
      "Epoch 3/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3860 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4047\n",
      "Epoch 4/100\n",
      "5/5 [==============================] - 1s 313ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3920 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4062\n",
      "Epoch 5/100\n",
      "5/5 [==============================] - 1s 315ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3940 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4067\n",
      "Epoch 6/100\n",
      "5/5 [==============================] - 1s 307ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3920 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3967\n",
      "Epoch 7/100\n",
      "5/5 [==============================] - 1s 323ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3740 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4018\n",
      "Epoch 8/100\n",
      "5/5 [==============================] - 1s 304ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3780 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4031\n",
      "Epoch 9/100\n",
      "5/5 [==============================] - 1s 330ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3800 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4055\n",
      "Epoch 10/100\n",
      "5/5 [==============================] - 1s 301ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3840 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4060\n",
      "Epoch 11/100\n",
      "5/5 [==============================] - 2s 411ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3840 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4070\n",
      "Epoch 12/100\n",
      "5/5 [==============================] - 2s 386ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3880 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4055\n",
      "Epoch 13/100\n",
      "5/5 [==============================] - 2s 432ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3840 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4050\n",
      "Epoch 14/100\n",
      "5/5 [==============================] - 2s 467ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3840 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4054\n",
      "Epoch 15/100\n",
      "5/5 [==============================] - 1s 335ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3800 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3894\n",
      "Epoch 16/100\n",
      "5/5 [==============================] - 1s 315ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3887\n",
      "Epoch 17/100\n",
      "5/5 [==============================] - 1s 317ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3860 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4055\n",
      "Epoch 18/100\n",
      "5/5 [==============================] - 1s 318ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3860 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4058\n",
      "Epoch 19/100\n",
      "5/5 [==============================] - 2s 398ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3820 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4058\n",
      "Epoch 20/100\n",
      "5/5 [==============================] - 2s 380ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3840 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.4058\n",
      "Epoch 21/100\n",
      "5/5 [==============================] - 2s 370ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3700 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3851\n",
      "Epoch 22/100\n",
      "5/5 [==============================] - 1s 332ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3680 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3851\n",
      "Epoch 23/100\n",
      "5/5 [==============================] - 1s 354ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3700 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3888\n",
      "Epoch 24/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3660 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3727\n",
      "Epoch 25/100\n",
      "5/5 [==============================] - 1s 324ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3720\n",
      "Epoch 26/100\n",
      "5/5 [==============================] - 1s 313ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3540 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3700\n",
      "Epoch 27/100\n",
      "5/5 [==============================] - 1s 320ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3540 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3698\n",
      "Epoch 28/100\n",
      "5/5 [==============================] - 1s 348ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3540 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3701\n",
      "Epoch 29/100\n",
      "5/5 [==============================] - 2s 420ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3727\n",
      "Epoch 30/100\n",
      "5/5 [==============================] - 2s 374ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3720\n",
      "Epoch 31/100\n",
      "5/5 [==============================] - 1s 336ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3723\n",
      "Epoch 32/100\n",
      "5/5 [==============================] - 1s 291ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3540 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3645\n",
      "Epoch 33/100\n",
      "5/5 [==============================] - 1s 321ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3580 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3646\n",
      "Epoch 34/100\n",
      "5/5 [==============================] - 1s 302ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3560 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3685\n",
      "Epoch 35/100\n",
      "5/5 [==============================] - 1s 287ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3580 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3687\n",
      "Epoch 36/100\n",
      "5/5 [==============================] - 1s 272ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3580 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3687\n",
      "Epoch 37/100\n",
      "5/5 [==============================] - 2s 417ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3580 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3688\n",
      "Epoch 38/100\n",
      "5/5 [==============================] - 1s 268ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3580 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 39/100\n",
      "5/5 [==============================] - 1s 314ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 40/100\n",
      "5/5 [==============================] - 1s 278ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3695\n",
      "Epoch 41/100\n",
      "5/5 [==============================] - 1s 274ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3695\n",
      "Epoch 42/100\n",
      "5/5 [==============================] - 1s 289ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3695\n",
      "Epoch 43/100\n",
      "5/5 [==============================] - 1s 285ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3695\n",
      "Epoch 44/100\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3687\n",
      "Epoch 45/100\n",
      "5/5 [==============================] - 1s 275ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3600 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 46/100\n",
      "5/5 [==============================] - 1s 298ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 47/100\n",
      "5/5 [==============================] - 1s 285ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 48/100\n",
      "5/5 [==============================] - 1s 279ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 49/100\n",
      "5/5 [==============================] - 1s 290ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 50/100\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 51/100\n",
      "5/5 [==============================] - 1s 309ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 52/100\n",
      "5/5 [==============================] - 1s 310ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 53/100\n",
      "5/5 [==============================] - 1s 292ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3694\n",
      "Epoch 54/100\n",
      "5/5 [==============================] - 1s 280ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 55/100\n",
      "5/5 [==============================] - 1s 280ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 56/100\n",
      "5/5 [==============================] - 1s 286ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 57/100\n",
      "5/5 [==============================] - 2s 364ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 58/100\n",
      "5/5 [==============================] - 1s 275ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 59/100\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 60/100\n",
      "5/5 [==============================] - 1s 293ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 61/100\n",
      "5/5 [==============================] - 1s 278ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 62/100\n",
      "5/5 [==============================] - 1s 268ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 63/100\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 64/100\n",
      "5/5 [==============================] - 1s 268ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 65/100\n",
      "5/5 [==============================] - 1s 287ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 66/100\n",
      "5/5 [==============================] - 1s 286ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 67/100\n",
      "5/5 [==============================] - 1s 277ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 68/100\n",
      "5/5 [==============================] - 1s 303ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 69/100\n",
      "5/5 [==============================] - 2s 436ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 70/100\n",
      "5/5 [==============================] - 2s 511ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 71/100\n",
      "5/5 [==============================] - 1s 265ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 72/100\n",
      "5/5 [==============================] - 1s 261ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 73/100\n",
      "5/5 [==============================] - 2s 586ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 74/100\n",
      "5/5 [==============================] - 4s 1s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 75/100\n",
      "5/5 [==============================] - 3s 815ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 76/100\n",
      "5/5 [==============================] - 5s 1s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 77/100\n",
      "5/5 [==============================] - 2346s 587s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 78/100\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 79/100\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 80/100\n",
      "5/5 [==============================] - 19s 5s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 81/100\n",
      "5/5 [==============================] - 10s 2s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 82/100\n",
      "5/5 [==============================] - 4s 870ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 83/100\n",
      "5/5 [==============================] - 3s 774ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 84/100\n",
      "5/5 [==============================] - 3s 783ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 85/100\n",
      "5/5 [==============================] - 3s 681ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 86/100\n",
      "5/5 [==============================] - 3s 668ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 87/100\n",
      "5/5 [==============================] - 2s 454ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 88/100\n",
      "5/5 [==============================] - 2s 509ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 89/100\n",
      "5/5 [==============================] - 2s 405ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 90/100\n",
      "5/5 [==============================] - 2s 606ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 91/100\n",
      "5/5 [==============================] - 1s 356ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 92/100\n",
      "5/5 [==============================] - 1s 363ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 93/100\n",
      "5/5 [==============================] - 1s 305ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 94/100\n",
      "5/5 [==============================] - 1s 308ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 95/100\n",
      "5/5 [==============================] - 2168s 542s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 96/100\n",
      "5/5 [==============================] - 11s 2s/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 97/100\n",
      "5/5 [==============================] - 2s 481ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 98/100\n",
      "5/5 [==============================] - 2s 505ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 99/100\n",
      "5/5 [==============================] - 2s 466ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n",
      "Epoch 100/100\n",
      "5/5 [==============================] - 2s 435ms/step - loss: 0.0000e+00 - binary_accuracy: 0.3620 - val_loss: 0.0000e+00 - val_binary_accuracy: 0.3690\n"
     ]
    }
   ],
   "source": [
    "history=  model.fit (partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=100,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The best accuracy obtained was 0.77 \n",
    "2. The accuracy at the dev set stalls at 70 epochs, thus we could have stopped there. \n",
    "3. thee difference is reather small, this means that there is virtually no overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: A Deeper Network\n",
    "Experiment with a network that has one hidden dense layer with a `'relu'` activation. The resulting system is no longer a logistic regression classifier, it's something more complex. Try the following sizes in the hidden layer:\n",
    "\n",
    "* 5, 7, 10\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. Which system performed best on the dev-test set?\n",
    "2. Would you add more or less cells in the hidden layer? Justify your answer.\n",
    "3. Is this system better than the simpler system of the previous exercise? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: Deep Learning with the Movie Review Corpus\n",
    "The notebook [W04L1-2-MovieReviews.ipynb](../lectures/W04L1-2-MovieReviews.ipynb) has several questions at the end, repeated below. Try to answer these, and indeed try other variants!\n",
    "\n",
    "* We were using 2 hidden layers. Try to use 1 or 3 hidden layers and see how it affects validation and test accuracy.\n",
    "* Try to use layers with more hidden units or less hidden units: 32 units, 64 units...\n",
    "* Try to use the `mse` loss function instead of `binary_crossentropy`.\n",
    "* Try to use the `tanh` activation (an activation that was popular in the early days of neural networks) instead of `relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
